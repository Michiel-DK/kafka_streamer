{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 13:46:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "INFO:root:Spark session initialized successfully.\n",
      "24/09/26 13:46:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 73770, test: 18226\n"
     ]
    }
   ],
   "source": [
    "from btc_streamer.ml.preprocessing import BTCDataloader\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"XGBoostSparkTest\") \\\n",
    ".getOrCreate()\n",
    "logging.info(\"Spark session initialized successfully.\")\n",
    "\n",
    "# Load and prepare data\n",
    "btc = BTCDataloader()\n",
    "btc.setup_spark()\n",
    "df = btc.load_data(directory='../data/')\n",
    "train, test, spark_session = btc.preproc_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = SparkXGBClassifier(features_col= \"features\", \n",
    "                        label_col=\"target\", \n",
    "                        num_workers = 4, \n",
    "                        random_state = 1,\n",
    "                        missing = None, \n",
    "                        validation_indicator_col = 'isVal',\n",
    "                        eval_metric = 'aucpr'    \n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.setParams(early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_input_kwargs': {'features_col': 'features',\n",
       "  'label_col': 'target',\n",
       "  'num_workers': 4,\n",
       "  'random_state': 1,\n",
       "  'missing': None,\n",
       "  'validation_indicator_col': 'isVal',\n",
       "  'eval_metric': 'aucpr'},\n",
       " 'uid': 'SparkXGBClassifier_6145cfee0a33',\n",
       " '_paramMap': {Param(parent='SparkXGBClassifier_6145cfee0a33', name='featuresCol', doc='features column name.'): 'features',\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='labelCol', doc='label column name.'): 'target',\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='num_workers', doc='The number of XGBoost workers. Each XGBoost worker corresponds to one spark task.'): 4,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='random_state', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param random_state'): 1,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='missing', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param missing'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='validationIndicatorCol', doc='name of the column that indicates whether each row is for training or for validation. False indicates training; true indicates validation.'): 'isVal',\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='eval_metric', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param eval_metric'): 'aucpr',\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='arbitrary_params_dict', doc='arbitrary_params_dict This parameter holds all of the additional parameters which are not exposed as the XGBoost Spark estimator params but can be recognized by underlying XGBoost library. It is stored as a dictionary.'): {},\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='early_stopping_rounds', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param early_stopping_rounds'): 5},\n",
       " '_defaultParamMap': {Param(parent='SparkXGBClassifier_6145cfee0a33', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='enable_sparse_data_optim', doc='This stores the boolean config of enabling sparse data optimization, if enabled, Xgboost DMatrix object will be constructed from sparse matrix instead of dense matrix. This config is disabled by default. If most of examples in your training dataset contains sparse features, we suggest to enable this config.'): False,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='features_cols', doc='feature column names.'): [],\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='labelCol', doc='label column name.'): 'label',\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='featuresCol', doc='features column name.'): 'features',\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='objective', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param objective'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='base_score', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param base_score'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='booster', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param booster'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='callbacks', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param callbacks'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='colsample_bylevel', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param colsample_bylevel'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='colsample_bynode', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param colsample_bynode'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='colsample_bytree', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param colsample_bytree'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='device', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param device'): 'cpu',\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='early_stopping_rounds', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param early_stopping_rounds'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='eval_metric', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param eval_metric'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='feature_types', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param feature_types'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='gamma', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param gamma'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='grow_policy', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param grow_policy'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='importance_type', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param importance_type'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='interaction_constraints', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param interaction_constraints'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='learning_rate', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param learning_rate'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_bin', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_bin'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_cat_threshold', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_cat_threshold'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_cat_to_onehot', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_cat_to_onehot'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_delta_step', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_delta_step'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_depth', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_depth'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_leaves', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_leaves'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='min_child_weight', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param min_child_weight'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='missing', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param missing'): nan,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='monotone_constraints', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param monotone_constraints'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='multi_strategy', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param multi_strategy'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='n_estimators', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param n_estimators'): 100,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='num_parallel_tree', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param num_parallel_tree'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='random_state', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param random_state'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='reg_alpha', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param reg_alpha'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='reg_lambda', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param reg_lambda'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='sampling_method', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param sampling_method'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='scale_pos_weight', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param scale_pos_weight'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='subsample', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param subsample'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='tree_method', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param tree_method'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='validate_parameters', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param validate_parameters'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='verbosity', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param verbosity'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='verbose', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param verbose'): True,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='xgb_model', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param xgb_model'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='feature_weights', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param feature_weights'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='iteration_range', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.predict() for this param iteration_range'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='num_workers', doc='The number of XGBoost workers. Each XGBoost worker corresponds to one spark task.'): 1,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='use_gpu', doc='Deprecated, use `device` instead. A boolean variable. Set use_gpu=true if the executors are running on GPU instances. Currently, only one GPU per task is supported.'): False,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='force_repartition', doc='A boolean variable. Set force_repartition=true if you want to force the input dataset to be repartitioned before XGBoost training.Note: The auto repartitioning judgement is not fully accurate, so it is recommendedto have force_repartition be True.'): False,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='repartition_random_shuffle', doc='A boolean variable. Set repartition_random_shuffle=true if you want to random shuffle dataset when repartitioning is required. By default is True.'): False,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='feature_names', doc='A list of str to specify feature names.'): None,\n",
       "  Param(parent='SparkXGBClassifier_6145cfee0a33', name='arbitrary_params_dict', doc='arbitrary_params_dict This parameter holds all of the additional parameters which are not exposed as the XGBoost Spark estimator params but can be recognized by underlying XGBoost library. It is stored as a dictionary.'): {}},\n",
       " '_params': None,\n",
       " 'arbitrary_params_dict': Param(parent='SparkXGBClassifier_6145cfee0a33', name='arbitrary_params_dict', doc='arbitrary_params_dict This parameter holds all of the additional parameters which are not exposed as the XGBoost Spark estimator params but can be recognized by underlying XGBoost library. It is stored as a dictionary.'),\n",
       " 'base_margin_col': Param(parent='SparkXGBClassifier_6145cfee0a33', name='base_margin_col', doc='This stores the name for the column of the base margin'),\n",
       " 'base_score': Param(parent='SparkXGBClassifier_6145cfee0a33', name='base_score', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param base_score'),\n",
       " 'booster': Param(parent='SparkXGBClassifier_6145cfee0a33', name='booster', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param booster'),\n",
       " 'callbacks': Param(parent='SparkXGBClassifier_6145cfee0a33', name='callbacks', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param callbacks'),\n",
       " 'colsample_bylevel': Param(parent='SparkXGBClassifier_6145cfee0a33', name='colsample_bylevel', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param colsample_bylevel'),\n",
       " 'colsample_bynode': Param(parent='SparkXGBClassifier_6145cfee0a33', name='colsample_bynode', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param colsample_bynode'),\n",
       " 'colsample_bytree': Param(parent='SparkXGBClassifier_6145cfee0a33', name='colsample_bytree', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param colsample_bytree'),\n",
       " 'device': Param(parent='SparkXGBClassifier_6145cfee0a33', name='device', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param device'),\n",
       " 'early_stopping_rounds': Param(parent='SparkXGBClassifier_6145cfee0a33', name='early_stopping_rounds', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param early_stopping_rounds'),\n",
       " 'enable_sparse_data_optim': Param(parent='SparkXGBClassifier_6145cfee0a33', name='enable_sparse_data_optim', doc='This stores the boolean config of enabling sparse data optimization, if enabled, Xgboost DMatrix object will be constructed from sparse matrix instead of dense matrix. This config is disabled by default. If most of examples in your training dataset contains sparse features, we suggest to enable this config.'),\n",
       " 'eval_metric': Param(parent='SparkXGBClassifier_6145cfee0a33', name='eval_metric', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param eval_metric'),\n",
       " 'feature_names': Param(parent='SparkXGBClassifier_6145cfee0a33', name='feature_names', doc='A list of str to specify feature names.'),\n",
       " 'feature_types': Param(parent='SparkXGBClassifier_6145cfee0a33', name='feature_types', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param feature_types'),\n",
       " 'feature_weights': Param(parent='SparkXGBClassifier_6145cfee0a33', name='feature_weights', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param feature_weights'),\n",
       " 'featuresCol': Param(parent='SparkXGBClassifier_6145cfee0a33', name='featuresCol', doc='features column name.'),\n",
       " 'features_cols': Param(parent='SparkXGBClassifier_6145cfee0a33', name='features_cols', doc='feature column names.'),\n",
       " 'force_repartition': Param(parent='SparkXGBClassifier_6145cfee0a33', name='force_repartition', doc='A boolean variable. Set force_repartition=true if you want to force the input dataset to be repartitioned before XGBoost training.Note: The auto repartitioning judgement is not fully accurate, so it is recommendedto have force_repartition be True.'),\n",
       " 'gamma': Param(parent='SparkXGBClassifier_6145cfee0a33', name='gamma', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param gamma'),\n",
       " 'grow_policy': Param(parent='SparkXGBClassifier_6145cfee0a33', name='grow_policy', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param grow_policy'),\n",
       " 'importance_type': Param(parent='SparkXGBClassifier_6145cfee0a33', name='importance_type', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param importance_type'),\n",
       " 'interaction_constraints': Param(parent='SparkXGBClassifier_6145cfee0a33', name='interaction_constraints', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param interaction_constraints'),\n",
       " 'iteration_range': Param(parent='SparkXGBClassifier_6145cfee0a33', name='iteration_range', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.predict() for this param iteration_range'),\n",
       " 'labelCol': Param(parent='SparkXGBClassifier_6145cfee0a33', name='labelCol', doc='label column name.'),\n",
       " 'learning_rate': Param(parent='SparkXGBClassifier_6145cfee0a33', name='learning_rate', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param learning_rate'),\n",
       " 'max_bin': Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_bin', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_bin'),\n",
       " 'max_cat_threshold': Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_cat_threshold', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_cat_threshold'),\n",
       " 'max_cat_to_onehot': Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_cat_to_onehot', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_cat_to_onehot'),\n",
       " 'max_delta_step': Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_delta_step', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_delta_step'),\n",
       " 'max_depth': Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_depth', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_depth'),\n",
       " 'max_leaves': Param(parent='SparkXGBClassifier_6145cfee0a33', name='max_leaves', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param max_leaves'),\n",
       " 'min_child_weight': Param(parent='SparkXGBClassifier_6145cfee0a33', name='min_child_weight', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param min_child_weight'),\n",
       " 'missing': Param(parent='SparkXGBClassifier_6145cfee0a33', name='missing', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param missing'),\n",
       " 'monotone_constraints': Param(parent='SparkXGBClassifier_6145cfee0a33', name='monotone_constraints', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param monotone_constraints'),\n",
       " 'multi_strategy': Param(parent='SparkXGBClassifier_6145cfee0a33', name='multi_strategy', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param multi_strategy'),\n",
       " 'n_estimators': Param(parent='SparkXGBClassifier_6145cfee0a33', name='n_estimators', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param n_estimators'),\n",
       " 'num_parallel_tree': Param(parent='SparkXGBClassifier_6145cfee0a33', name='num_parallel_tree', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param num_parallel_tree'),\n",
       " 'num_workers': Param(parent='SparkXGBClassifier_6145cfee0a33', name='num_workers', doc='The number of XGBoost workers. Each XGBoost worker corresponds to one spark task.'),\n",
       " 'objective': Param(parent='SparkXGBClassifier_6145cfee0a33', name='objective', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param objective'),\n",
       " 'pred_contrib_col': Param(parent='SparkXGBClassifier_6145cfee0a33', name='pred_contrib_col', doc='feature contributions to individual predictions.'),\n",
       " 'predictionCol': Param(parent='SparkXGBClassifier_6145cfee0a33', name='predictionCol', doc='prediction column name.'),\n",
       " 'probabilityCol': Param(parent='SparkXGBClassifier_6145cfee0a33', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'),\n",
       " 'qid_col': Param(parent='SparkXGBClassifier_6145cfee0a33', name='qid_col', doc='query id column name'),\n",
       " 'random_state': Param(parent='SparkXGBClassifier_6145cfee0a33', name='random_state', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param random_state'),\n",
       " 'rawPredictionCol': Param(parent='SparkXGBClassifier_6145cfee0a33', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'),\n",
       " 'reg_alpha': Param(parent='SparkXGBClassifier_6145cfee0a33', name='reg_alpha', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param reg_alpha'),\n",
       " 'reg_lambda': Param(parent='SparkXGBClassifier_6145cfee0a33', name='reg_lambda', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param reg_lambda'),\n",
       " 'repartition_random_shuffle': Param(parent='SparkXGBClassifier_6145cfee0a33', name='repartition_random_shuffle', doc='A boolean variable. Set repartition_random_shuffle=true if you want to random shuffle dataset when repartitioning is required. By default is True.'),\n",
       " 'sampling_method': Param(parent='SparkXGBClassifier_6145cfee0a33', name='sampling_method', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param sampling_method'),\n",
       " 'scale_pos_weight': Param(parent='SparkXGBClassifier_6145cfee0a33', name='scale_pos_weight', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param scale_pos_weight'),\n",
       " 'subsample': Param(parent='SparkXGBClassifier_6145cfee0a33', name='subsample', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param subsample'),\n",
       " 'tree_method': Param(parent='SparkXGBClassifier_6145cfee0a33', name='tree_method', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param tree_method'),\n",
       " 'use_gpu': Param(parent='SparkXGBClassifier_6145cfee0a33', name='use_gpu', doc='Deprecated, use `device` instead. A boolean variable. Set use_gpu=true if the executors are running on GPU instances. Currently, only one GPU per task is supported.'),\n",
       " 'validate_parameters': Param(parent='SparkXGBClassifier_6145cfee0a33', name='validate_parameters', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param validate_parameters'),\n",
       " 'validationIndicatorCol': Param(parent='SparkXGBClassifier_6145cfee0a33', name='validationIndicatorCol', doc='name of the column that indicates whether each row is for training or for validation. False indicates training; true indicates validation.'),\n",
       " 'verbose': Param(parent='SparkXGBClassifier_6145cfee0a33', name='verbose', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param verbose'),\n",
       " 'verbosity': Param(parent='SparkXGBClassifier_6145cfee0a33', name='verbosity', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier for this param verbosity'),\n",
       " 'weightCol': Param(parent='SparkXGBClassifier_6145cfee0a33', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.'),\n",
       " 'xgb_model': Param(parent='SparkXGBClassifier_6145cfee0a33', name='xgb_model', doc='Refer to XGBoost doc of xgboost.sklearn.XGBClassifier.fit() for this param xgb_model'),\n",
       " 'logger': <Logger SparkXGBClassifier (INFO)>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|target|\n",
      "+--------------------+------+\n",
      "|[-0.0935832341819...|     0|\n",
      "|[-0.0800532148741...|     0|\n",
      "|[-0.0697162404845...|     0|\n",
      "|[-0.0660354893981...|     0|\n",
      "|[-0.0650725049758...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `isVal` cannot be resolved. Did you mean one of the following? [`target`, `features`].;\n'Project [target#395 AS label#496, UDF(features#425) AS values#501, 'isVal AS validationIndicator#502]\n+- Sample 0.0, 0.8, false, 42\n   +- Sort [features#425 ASC NULLS FIRST, target#395 ASC NULLS FIRST], false\n      +- Sample 0.0, 0.2, false, 42\n         +- Sort [features#425 ASC NULLS FIRST, target#395 ASC NULLS FIRST], false\n            +- Project [features#425, target#395]\n               +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, percent_change_24h#382, target#395, UDF(struct(percent_change_30m, percent_change_30m#340, percent_change_1h, percent_change_1h#349, percent_change_6h, percent_change_6h#359, percent_change_12h, percent_change_12h#370, percent_change_24h, percent_change_24h#382)) AS features#425]\n                  +- Filter atleastnnonnulls(9, timestamp#324, price#316, Volume_USD#325, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, percent_change_24h#382, target#395)\n                     +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, percent_change_24h#382, target#395]\n                        +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, percent_change_24h#382, CASE WHEN (percent_change_15m#332 > cast(0 as double)) THEN 1 ELSE 0 END AS target#395]\n                           +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, percent_change_24h#382]\n                              +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, month#331, window#383, _we0#384, _we1#385, ((price#316 - _we0#384) / _we1#385) AS percent_change_24h#382]\n                                 +- Window [lag(price#316, -288, null) windowspecdefinition(window#383, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -288, -288)) AS _we0#384, lag(price#316, -288, null) windowspecdefinition(window#383, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -288, -288)) AS _we1#385], [window#383], [timestamp#324 ASC NULLS FIRST]\n                                    +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, window#383 AS month#331, window#383]\n                                       +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#383, timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370]\n                                          +- Filter isnotnull(timestamp#324)\n                                             +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370]\n                                                +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, month#331, window#371, _we0#372, _we1#373, ((price#316 - _we0#372) / _we1#373) AS percent_change_12h#370]\n                                                   +- Window [lag(price#316, -144, null) windowspecdefinition(window#371, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -144, -144)) AS _we0#372, lag(price#316, -144, null) windowspecdefinition(window#371, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -144, -144)) AS _we1#373], [window#371], [timestamp#324 ASC NULLS FIRST]\n                                                      +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, window#371 AS month#331, window#371]\n                                                         +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#371, timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359]\n                                                            +- Filter isnotnull(timestamp#324)\n                                                               +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359]\n                                                                  +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, month#331, window#360, _we0#361, _we1#362, ((price#316 - _we0#361) / _we1#362) AS percent_change_6h#359]\n                                                                     +- Window [lag(price#316, -72, null) windowspecdefinition(window#360, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -72, -72)) AS _we0#361, lag(price#316, -72, null) windowspecdefinition(window#360, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -72, -72)) AS _we1#362], [window#360], [timestamp#324 ASC NULLS FIRST]\n                                                                        +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, window#360 AS month#331, window#360]\n                                                                           +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#360, timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349]\n                                                                              +- Filter isnotnull(timestamp#324)\n                                                                                 +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349]\n                                                                                    +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, month#331, window#350, _we0#351, _we1#352, ((price#316 - _we0#351) / _we1#352) AS percent_change_1h#349]\n                                                                                       +- Window [lag(price#316, -12, null) windowspecdefinition(window#350, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -12, -12)) AS _we0#351, lag(price#316, -12, null) windowspecdefinition(window#350, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -12, -12)) AS _we1#352], [window#350], [timestamp#324 ASC NULLS FIRST]\n                                                                                          +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, window#350 AS month#331, window#350]\n                                                                                             +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#350, timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340]\n                                                                                                +- Filter isnotnull(timestamp#324)\n                                                                                                   +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340]\n                                                                                                      +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, month#331, window#341, _we0#342, _we1#343, ((price#316 - _we0#342) / _we1#343) AS percent_change_30m#340]\n                                                                                                         +- Window [lag(price#316, -6, null) windowspecdefinition(window#341, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -6, -6)) AS _we0#342, lag(price#316, -6, null) windowspecdefinition(window#341, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -6, -6)) AS _we1#343], [window#341], [timestamp#324 ASC NULLS FIRST]\n                                                                                                            +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, window#341 AS month#331, window#341]\n                                                                                                               +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#341, timestamp#324, price#316, Volume_USD#325, percent_change_15m#332]\n                                                                                                                  +- Filter isnotnull(timestamp#324)\n                                                                                                                     +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332]\n                                                                                                                        +- Project [timestamp#324, price#316, Volume_USD#325, month#331, window#333, _we0#334, _we1#335, ((price#316 - _we0#334) / _we1#335) AS percent_change_15m#332]\n                                                                                                                           +- Window [lag(price#316, -3, null) windowspecdefinition(window#333, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -3, -3)) AS _we0#334, lag(price#316, -3, null) windowspecdefinition(window#333, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -3, -3)) AS _we1#335], [window#333], [timestamp#324 ASC NULLS FIRST]\n                                                                                                                              +- Project [timestamp#324, price#316, Volume_USD#325, window#333 AS month#331, window#333]\n                                                                                                                                 +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#333, timestamp#324, price#316, Volume_USD#325]\n                                                                                                                                    +- Filter isnotnull(timestamp#324)\n                                                                                                                                       +- Project [time_window#305.end AS timestamp#324, price#316, volume_5m#318 AS Volume_USD#325]\n                                                                                                                                          +- Sort [time_window#305.end ASC NULLS FIRST], true\n                                                                                                                                             +- Aggregate [window#319], [window#319 AS time_window#305, last(close#271, false) AS price#316, sum(Volume USD#273) AS volume_5m#318]\n                                                                                                                                                +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#283, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#283, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) END) - 0) + 300000000), LongType, TimestampType))) AS window#319, unix#265, symbol#267, open#268, high#269, low#270, close#271, Volume BTC#272, Volume USD#273, timestamp#283]\n                                                                                                                                                   +- Filter isnotnull(timestamp#283)\n                                                                                                                                                      +- Project [unix#265, symbol#267, open#268, high#269, low#270, close#271, Volume BTC#272, Volume USD#273, timestamp#283]\n                                                                                                                                                         +- Sort [timestamp#283 ASC NULLS FIRST], true\n                                                                                                                                                            +- Project [unix#265, date#266, symbol#267, open#268, high#269, low#270, close#271, Volume BTC#272, Volume USD#273, cast(from_unixtime(cast(unix#265 as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Brussels)) as timestamp) AS timestamp#283]\n                                                                                                                                                               +- Relation [unix#265,date#266,symbol#267,open#268,high#269,low#270,close#271,Volume BTC#272,Volume USD#273] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/xgboost/spark/core.py:1003\u001b[0m, in \u001b[0;36m_SparkXGBEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_SparkXGBModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# pylint: disable=too-many-statements, too-many-locals\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m-> 1003\u001b[0m     dataset, feature_prop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m     (\n\u001b[1;32m   1006\u001b[0m         booster_params,\n\u001b[1;32m   1007\u001b[0m         train_call_kwargs_params,\n\u001b[1;32m   1008\u001b[0m         dmatrix_kwargs,\n\u001b[1;32m   1009\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_xgb_parameters(dataset)\n\u001b[1;32m   1011\u001b[0m     run_on_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_on_gpu()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/xgboost/spark/core.py:824\u001b[0m, in \u001b[0;36m_SparkXGBEstimator._prepare_input\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepare the input including column pruning, repartition and so on\"\"\"\u001b[39;00m\n\u001b[1;32m    820\u001b[0m select_cols, feature_prop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input_columns_and_feature_prop(\n\u001b[1;32m    821\u001b[0m     dataset\n\u001b[1;32m    822\u001b[0m )\n\u001b[0;32m--> 824\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mselect_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetOrDefault(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers)\n\u001b[1;32m    827\u001b[0m sc \u001b[38;5;241m=\u001b[39m _get_spark_session()\u001b[38;5;241m.\u001b[39msparkContext\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[1;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `isVal` cannot be resolved. Did you mean one of the following? [`target`, `features`].;\n'Project [target#395 AS label#496, UDF(features#425) AS values#501, 'isVal AS validationIndicator#502]\n+- Sample 0.0, 0.8, false, 42\n   +- Sort [features#425 ASC NULLS FIRST, target#395 ASC NULLS FIRST], false\n      +- Sample 0.0, 0.2, false, 42\n         +- Sort [features#425 ASC NULLS FIRST, target#395 ASC NULLS FIRST], false\n            +- Project [features#425, target#395]\n               +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, percent_change_24h#382, target#395, UDF(struct(percent_change_30m, percent_change_30m#340, percent_change_1h, percent_change_1h#349, percent_change_6h, percent_change_6h#359, percent_change_12h, percent_change_12h#370, percent_change_24h, percent_change_24h#382)) AS features#425]\n                  +- Filter atleastnnonnulls(9, timestamp#324, price#316, Volume_USD#325, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, percent_change_24h#382, target#395)\n                     +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, percent_change_24h#382, target#395]\n                        +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, percent_change_24h#382, CASE WHEN (percent_change_15m#332 > cast(0 as double)) THEN 1 ELSE 0 END AS target#395]\n                           +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, percent_change_24h#382]\n                              +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, month#331, window#383, _we0#384, _we1#385, ((price#316 - _we0#384) / _we1#385) AS percent_change_24h#382]\n                                 +- Window [lag(price#316, -288, null) windowspecdefinition(window#383, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -288, -288)) AS _we0#384, lag(price#316, -288, null) windowspecdefinition(window#383, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -288, -288)) AS _we1#385], [window#383], [timestamp#324 ASC NULLS FIRST]\n                                    +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370, window#383 AS month#331, window#383]\n                                       +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#383, timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370]\n                                          +- Filter isnotnull(timestamp#324)\n                                             +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, percent_change_12h#370]\n                                                +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, month#331, window#371, _we0#372, _we1#373, ((price#316 - _we0#372) / _we1#373) AS percent_change_12h#370]\n                                                   +- Window [lag(price#316, -144, null) windowspecdefinition(window#371, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -144, -144)) AS _we0#372, lag(price#316, -144, null) windowspecdefinition(window#371, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -144, -144)) AS _we1#373], [window#371], [timestamp#324 ASC NULLS FIRST]\n                                                      +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359, window#371 AS month#331, window#371]\n                                                         +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#371, timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359]\n                                                            +- Filter isnotnull(timestamp#324)\n                                                               +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, percent_change_6h#359]\n                                                                  +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, month#331, window#360, _we0#361, _we1#362, ((price#316 - _we0#361) / _we1#362) AS percent_change_6h#359]\n                                                                     +- Window [lag(price#316, -72, null) windowspecdefinition(window#360, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -72, -72)) AS _we0#361, lag(price#316, -72, null) windowspecdefinition(window#360, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -72, -72)) AS _we1#362], [window#360], [timestamp#324 ASC NULLS FIRST]\n                                                                        +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349, window#360 AS month#331, window#360]\n                                                                           +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#360, timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349]\n                                                                              +- Filter isnotnull(timestamp#324)\n                                                                                 +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, percent_change_1h#349]\n                                                                                    +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, month#331, window#350, _we0#351, _we1#352, ((price#316 - _we0#351) / _we1#352) AS percent_change_1h#349]\n                                                                                       +- Window [lag(price#316, -12, null) windowspecdefinition(window#350, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -12, -12)) AS _we0#351, lag(price#316, -12, null) windowspecdefinition(window#350, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -12, -12)) AS _we1#352], [window#350], [timestamp#324 ASC NULLS FIRST]\n                                                                                          +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340, window#350 AS month#331, window#350]\n                                                                                             +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#350, timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340]\n                                                                                                +- Filter isnotnull(timestamp#324)\n                                                                                                   +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, percent_change_30m#340]\n                                                                                                      +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, month#331, window#341, _we0#342, _we1#343, ((price#316 - _we0#342) / _we1#343) AS percent_change_30m#340]\n                                                                                                         +- Window [lag(price#316, -6, null) windowspecdefinition(window#341, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -6, -6)) AS _we0#342, lag(price#316, -6, null) windowspecdefinition(window#341, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -6, -6)) AS _we1#343], [window#341], [timestamp#324 ASC NULLS FIRST]\n                                                                                                            +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332, window#341 AS month#331, window#341]\n                                                                                                               +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#341, timestamp#324, price#316, Volume_USD#325, percent_change_15m#332]\n                                                                                                                  +- Filter isnotnull(timestamp#324)\n                                                                                                                     +- Project [timestamp#324, price#316, Volume_USD#325, percent_change_15m#332]\n                                                                                                                        +- Project [timestamp#324, price#316, Volume_USD#325, month#331, window#333, _we0#334, _we1#335, ((price#316 - _we0#334) / _we1#335) AS percent_change_15m#332]\n                                                                                                                           +- Window [lag(price#316, -3, null) windowspecdefinition(window#333, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -3, -3)) AS _we0#334, lag(price#316, -3, null) windowspecdefinition(window#333, timestamp#324 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -3, -3)) AS _we1#335], [window#333], [timestamp#324 ASC NULLS FIRST]\n                                                                                                                              +- Project [timestamp#324, price#316, Volume_USD#325, window#333 AS month#331, window#333]\n                                                                                                                                 +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#324, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) + 604800000000) ELSE ((precisetimestampconversion(timestamp#324, TimestampType, LongType) - 0) % 604800000000) END) - 0) + 604800000000), LongType, TimestampType))) AS window#333, timestamp#324, price#316, Volume_USD#325]\n                                                                                                                                    +- Filter isnotnull(timestamp#324)\n                                                                                                                                       +- Project [time_window#305.end AS timestamp#324, price#316, volume_5m#318 AS Volume_USD#325]\n                                                                                                                                          +- Sort [time_window#305.end ASC NULLS FIRST], true\n                                                                                                                                             +- Aggregate [window#319], [window#319 AS time_window#305, last(close#271, false) AS price#316, sum(Volume USD#273) AS volume_5m#318]\n                                                                                                                                                +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#283, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#283, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#283, TimestampType, LongType) - 0) % 300000000) END) - 0) + 300000000), LongType, TimestampType))) AS window#319, unix#265, symbol#267, open#268, high#269, low#270, close#271, Volume BTC#272, Volume USD#273, timestamp#283]\n                                                                                                                                                   +- Filter isnotnull(timestamp#283)\n                                                                                                                                                      +- Project [unix#265, symbol#267, open#268, high#269, low#270, close#271, Volume BTC#272, Volume USD#273, timestamp#283]\n                                                                                                                                                         +- Sort [timestamp#283 ASC NULLS FIRST], true\n                                                                                                                                                            +- Project [unix#265, date#266, symbol#267, open#268, high#269, low#270, close#271, Volume BTC#272, Volume USD#273, cast(from_unixtime(cast(unix#265 as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Brussels)) as timestamp) AS timestamp#283]\n                                                                                                                                                               +- Relation [unix#265,date#266,symbol#267,open#268,high#269,low#270,close#271,Volume BTC#272,Volume USD#273] csv\n"
     ]
    }
   ],
   "source": [
    "model = xgb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_c6f9bd947d95"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert feature columns into a single vector column\n",
    "feature_columns = [x.name for x in df.schema if re.search(r'percent', x.name)]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features') \n",
    "\n",
    "assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_data = assembler.transform(df).select(['features', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = assembled_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary:logistic',  # Binary classification\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'auto',           # Ensure using CPU\n",
    "        'num_round': 100,\n",
    "        'eta': 0.1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_estimator = SparkXGBClassifier(\n",
    "        features_col='features',\n",
    "        label_col='target',use_gpu=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 12:38:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|target|\n",
      "+--------------------+------+\n",
      "|[-0.1015825605186...|     0|\n",
      "|[-0.0805882532746...|     0|\n",
      "|[-0.0750382016173...|     0|\n",
      "|[-0.0712359669925...|     0|\n",
      "|[-0.0697370117538...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 12:38:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/09/25 12:38:10 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 1 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.python.PythonFunction. Trace:\npy4j.Py4JException: Constructor org.apache.spark.api.python.PythonFunction([class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.String, class java.lang.String, class java.util.ArrayList, class org.apache.spark.api.python.PythonAccumulatorV2]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/xgboost/spark/core.py:1125\u001b[0m, in \u001b[0;36m_SparkXGBEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret[\u001b[38;5;241m0\u001b[39m], ret[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1114\u001b[0m get_logger(_LOG_TAG)\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning xgboost-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m workers with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mbooster params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     dmatrix_kwargs,\n\u001b[1;32m   1124\u001b[0m )\n\u001b[0;32m-> 1125\u001b[0m (config, booster) \u001b[38;5;241m=\u001b[39m \u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m get_logger(_LOG_TAG)\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished xgboost training!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1128\u001b[0m result_xgb_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_sklearn_model(\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(booster, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), config\n\u001b[1;32m   1130\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/xgboost/spark/core.py:1103\u001b[0m, in \u001b[0;36m_SparkXGBEstimator._fit.<locals>._run_job\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_job\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1102\u001b[0m     rdd \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1103\u001b[0m         \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapInPandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_train_booster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig string, booster string\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m         \u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mbarrier()\n\u001b[1;32m   1108\u001b[0m         \u001b[38;5;241m.\u001b[39mmapPartitions(\u001b[38;5;28;01mlambda\u001b[39;00m x: x)\n\u001b[1;32m   1109\u001b[0m     )\n\u001b[1;32m   1110\u001b[0m     rdd_with_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_stage_level_scheduling(rdd)\n\u001b[1;32m   1111\u001b[0m     ret \u001b[38;5;241m=\u001b[39m rdd_with_resource\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/sql/pandas/map_ops.py:81\u001b[0m, in \u001b[0;36mPandasMapOpsMixin.mapInPandas\u001b[0;34m(self, func, schema)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, DataFrame)\n\u001b[1;32m     79\u001b[0m udf \u001b[38;5;241m=\u001b[39m pandas_udf(\n\u001b[1;32m     80\u001b[0m     func, returnType\u001b[38;5;241m=\u001b[39mschema, functionType\u001b[38;5;241m=\u001b[39mPythonEvalType\u001b[38;5;241m.\u001b[39mSQL_MAP_PANDAS_ITER_UDF)\n\u001b[0;32m---> 81\u001b[0m udf_column \u001b[38;5;241m=\u001b[39m \u001b[43mudf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mmapInPandas(udf_column\u001b[38;5;241m.\u001b[39m_jc\u001b[38;5;241m.\u001b[39mexpr())\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/sql/udf.py:199\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, assigned\u001b[38;5;241m=\u001b[39massignments)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/sql/udf.py:177\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[0;32m--> 177\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_judf\u001b[49m\n\u001b[1;32m    178\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(judf\u001b[38;5;241m.\u001b[39mapply(_to_seq(sc, cols, _to_java_column)))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/sql/udf.py:161\u001b[0m, in \u001b[0;36mUserDefinedFunction._judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_judf\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# It is possible that concurrent access, to newly created UDF,\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# will initialize multiple UserDefinedPythonFunctions.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# This is unlikely, doesn't affect correctness,\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# and should have a minimal performance impact.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_judf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/sql/udf.py:170\u001b[0m, in \u001b[0;36mUserDefinedFunction._create_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m    168\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[0;32m--> 170\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturnType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m jdt \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mparseDataType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnType\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    172\u001b[0m judf \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mexecution\u001b[38;5;241m.\u001b[39mpython\u001b[38;5;241m.\u001b[39mUserDefinedPythonFunction(\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, wrapped_func, jdt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/sql/udf.py:35\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, returnType)\u001b[0m\n\u001b[1;32m     33\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, returnType)\n\u001b[1;32m     34\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytearray\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpickled_command\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincludes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpythonExec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpythonVer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbroadcast_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_javaAccumulator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/py4j/java_gateway.py:1568\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1562\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1564\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1565\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1567\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1568\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1572\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/kafka_streamer/lib/python3.10/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.api.python.PythonFunction. Trace:\npy4j.Py4JException: Constructor org.apache.spark.api.python.PythonFunction([class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.String, class java.lang.String, class java.util.ArrayList, class org.apache.spark.api.python.PythonAccumulatorV2]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\n\n"
     ]
    }
   ],
   "source": [
    "model = xgb_estimator.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka_streamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
