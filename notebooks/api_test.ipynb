{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "def get_crypto_price():\n",
    "    url = f\"https://api.coinpaprika.com/v1/tickers/btc-bitcoin\"\n",
    "    \n",
    "    headers = {\n",
    "    'Accept-Encoding': 'gzip',\n",
    "    'Authorization': f'Bearer {os.getenv(\"COINCAP_API_KEY\")}',\n",
    "}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    data = response.json()\n",
    "    try:\n",
    "        diction = data['quotes']['USD'] \n",
    "        diction['symbol'] = data['symbol']\n",
    "        diction['beta_value'] = data['beta_value']\n",
    "        diction['timestamp'] = data['last_updated']\n",
    "        del diction['ath_price']\n",
    "        del diction['ath_date']\n",
    "        del diction['percent_from_price_ath']\n",
    "        return diction\n",
    "    except:\n",
    "        print(f\"Error retrieving price\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_crypto_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+-----------------+------------------+------------------+------+\n",
      "|percent_change_30m|percent_change_1h|percent_change_6h|percent_change_12h|percent_change_24h|target|\n",
      "+------------------+-----------------+-----------------+------------------+------------------+------+\n",
      "|             -0.03|             0.09|             0.74|              1.09|              1.49|     0|\n",
      "+------------------+-----------------+-----------------+------------------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, sum as _sum, last, window, from_unixtime, lag, round, when, lit\n",
    "\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"BTCData\").getOrCreate()\n",
    "\n",
    "# Creating a Pandas DataFrame to filter the required fields\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "# Filtering only the required percentage change columns\n",
    "filtered_df = df[['percent_change_15m', 'percent_change_30m', 'percent_change_1h', \n",
    "                  'percent_change_6h', 'percent_change_12h', 'percent_change_24h']]\n",
    "\n",
    "# Converting the Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(filtered_df)\n",
    "\n",
    "df_final = spark_df.withColumn(\n",
    "            \"target\",\n",
    "            when(col(\"percent_change_15m\") > 0, 1).otherwise(0)\n",
    "        ).drop('percent_change_15m')\n",
    "\n",
    "# Show the Spark DataFrame\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "\n",
    "      # Convert feature columns into a single vector column\n",
    "feature_columns = [x.name for x in df_final.schema if re.search(r'percent', x.name)]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "            \n",
    "assembled_data_train = assembler.transform(df_final).select(['features', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from btc_streamer.ml.model_utils import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/12 16:04:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = load_model('../models/xgboost_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 16:16:17,555 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[Stage 5:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+----------+--------------------+\n",
      "|            features|target|       rawPrediction|prediction|         probability|\n",
      "+--------------------+------+--------------------+----------+--------------------+\n",
      "|[-0.03,0.09,0.74,...|     0|[1.64740657806396...|       0.0|[0.83854019641876...|\n",
      "+--------------------+------+--------------------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.transform(assembled_data_train).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka_streamer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
